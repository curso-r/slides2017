---
title: "Extra"
author: "J"
date: "February 15, 2017"
output: revealjs::revealjs_presentation
---

## Big data 

### (by Hadley Wickham)

## Como saber se meu problema é big data?

Big data is extremely overhyped and not terribly well defined. Many people think they have big data, when they actually don't.

## Pontos de transição

* From in-memory to disk. If your data fits in memory, it's small data. 
    - And these days you can get 1 TB of ram, so even small data is big! 
    - Moving from in-memory to on-disk is an important transition because access speeds are so different.
    - You can do quite naive computations on in-memory data and it’ll be fast enough. 
    - You need to plan (and index) much more with on-disk data

## Pontos de transição

* From one computer to many computers. 
    - The next important threshold occurs when you data no longer fits on one disk on one computer.
    - Moving to a distributed environment makes computation much more challenging because you don’t have all the data needed for a computation in one place. 
    - Designing distributed algorithms is much harder, and you’re fundamentally limited by the way the data is split up between computers.

## Pontos de transição

* I personally believe it's impossible for one system to span from in-memory to on-disk to distributed. 

    - R is a fantastic environment for the rapid exploration of in-memory data, but there’s no elegant way to scale it to much larger datasets. 
    - Hadoop/spark works well when you have thousands of computers, but is incredible slow on just one machine. 
    - Fortunately, I don't think one system needs to solve all big data problems.

## Classes de problemas

1. Big data problems that are actually small data problems, once you have the right subset/sample/summary. 
    - Inventing numbers on the spot, I’d say 90% of big data problems fall into this category. 
    - To solve this problem you need a distributed database (like hive, impala, teradata etc), and a tool like `dplyr` to let you rapidly iterate to the right small dataset (which still might be gigabytes in size).

## Classes de problemas

2. Big data problems that are actually lots and lots of small data problems, 
    - e.g. you need to fit one model per individual for thousands of individuals. 
    - I’d say ~9% of big data problems fall into this category. 
    - This sort of problem is known as a trivially parallelisable problem and you need some way to distribute computation over multiple machines. 
    - The `foreach` package is a nice solution to this problem because it abstracts away the backend, allowing you to focus on the computation, not the details of distributing it.

## Classes de problemas

3. Finally, there are irretrievably big problems where you do need all the data, perhaps because you fitting a complex model. 
    - An example of this type of problem is recommender systems which really do benefit from lots of data because they need to recognise interactions that occur only rarely. 
    - These problems tend to be solved by dedicated systems specifically designed to solve a particular problem.

## Big Data no R? {.build}

O R não é uma ferramenta de Big Data. Mas o que podemos fazer?

- Usar linguagens de programação de baixo nível, principalmente C
- Usar pacotes que lêem as bases do disco rígido e não da memória RAM
- Tirar uma amostra
- Usar um computador com mais memória RAM

## Usar um computador com mais memória RAM

- Não tenho dinheiro para comprar um computador com muita memória RAM.
- Usar computador na Amazon
    - Veja [esse link](http://curso-r.github.io/criar-instancia-amazon-ec2.html) (está um pouco desatualizado).

## Usando a memória RAM de forma mais eficiente

- Família de pacotes `big*`
- Exemplos: `bigmemmory`, `bigglm`

## Exemplo com o base R

```{r, eval=FALSE}
n <- 1e6
y <- rnorm(n)
d <- data.frame(y = y, x1 = 2*y + runif(n), 
                x2 = 3*y + runif(n), 
                x3 = rnorm(n), x4 = rnorm(n))
modelo <- lm(y ~ x1*x2*x3*x4, data = d)
```

## Exemplo usando biglm

```{r, eval=FALSE}
library(biglm)
n <- 1e6
y <- rnorm(n)
d <- data.frame(y = y, x1 = 2*y + runif(n), 
                x2 = 3*y + runif(n), 
                x3 = rnorm(n), x4 = rnorm(n))
modelo <- biglm(y ~ x1*x2*x3*x4, data = d)
```

- para glm é a mesma coisa!!

`biglm` gera um objeto com tamanho menor, além de ser mais eficiente.

## Outras funções big

```{r, eval=FALSE}
library(biganalytics)
n <- 5*1e6
y <- rnorm(n)
m <- matrix(c(y, 2*y + runif(n), 3*y + runif(n), 
              rnorm(n), rnorm(n)), ncol = 5)
modelo <- bigkmeans(x = m, centers = 2, iter.max = 100)
```

## Estratégias para ler bases grandes

[Estratégias para ler bases grandes](http://stackoverflow.com/questions/1727772/quickly-reading-very-large-tables-as-dataframes-in-r)

- função `fread` do pacote `data.table`

## `fread` é muito fácil de usar

```{r, eval=FALSE}
download.file("http://stat-computing.org/dataexpo/2009/2008.csv.bz2",
              destfile="2008.csv.bz2")
system("bunzip2 2008.csv.bz2")
library(data.table)
system.time(DT <- fread("2008.csv"))
```

## `fread`

- já aloca a memória necessária para ler o arquivo
- estabelece os tipos de colunas antes de ler o arquivo

## Paralelização

uso do pacote `parallel` junto com o `plyr`

## Criando um cluster (backend)

```{r warning=FALSE, message=FALSE, eval=FALSE}
library(parallel)
library(foreach)
library(doParallel)
c <- makePSOCKcluster(2)
registerDoParallel(c)
```

## Usando o cluster por meio do pacote plyr

```{r warning=FALSE, message=FALSE, eval=FALSE}
library(plyr)
system.time(l_ply(1:5, .fun = function(x) {Sys.sleep(1)}, 
                  .parallel = TRUE))
```

```{r warning=FALSE, message=FALSE, eval=FALSE}
system.time(l_ply(1:5, .fun = function(x) {Sys.sleep(1)}, 
                  .parallel = FALSE))
stopCluster(c)
```

## Deep learning

- Veja [esse curso](https://br.udacity.com/course/deep-learning--ud730/).
- Comparação de [algoritmos de deep learning em R](http://www.rblog.uni-freiburg.de/2017/02/07/deep-learning-in-r/)
    - `MXNetR`: Feed-forward neural network, convolutional neural network (CNN)
    - `darch`: Restricted Boltzmann machine, deep belief network
    - `deepnet`: Feed-forward neural network, restricted Boltzmann machine, deep belief network, stacked autoencoders
    - `H2O`: Feed-forward neural network, deep autoencoders
    - `deepr`: Simplify some functions from H2O and deepnet packages
- R como uma cola.

